{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "RAW_DATA_DIR = Path(\"../data/raw\")\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "WEATHER_DIR = RAW_DATA_DIR\n",
    "RETAIL_FILE = RAW_DATA_DIR / \"retail_sales_dataset.csv\"\n",
    "HEADLINES_DIR = RAW_DATA_DIR / \"web\"\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR.absolute()}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a826ea",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748421b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "def load_weather_data():\n",
    "    weather_files = list(WEATHER_DIR.glob(\"*_weather_*.json\"))\n",
    "    weather_records = []\n",
    "    \n",
    "    for file_path in weather_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            record = {\n",
    "                'timestamp': data.get('dt'),\n",
    "                'date': pd.to_datetime(data.get('dt'), unit='s').normalize() if data.get('dt') else None,\n",
    "                'city': data.get('name'),\n",
    "                'country': data.get('sys', {}).get('country'),\n",
    "                'temperature': data.get('main', {}).get('temp'),\n",
    "                'feels_like': data.get('main', {}).get('feels_like'),\n",
    "                'humidity': data.get('main', {}).get('humidity'),\n",
    "                'pressure': data.get('main', {}).get('pressure'),\n",
    "                'weather_main': data.get('weather', [{}])[0].get('main'),\n",
    "                'weather_description': data.get('weather', [{}])[0].get('description'),\n",
    "                'wind_speed': data.get('wind', {}).get('speed'),\n",
    "                'wind_direction': data.get('wind', {}).get('deg'),\n",
    "                'clouds': data.get('clouds', {}).get('all'),\n",
    "                'source_file': file_path.name\n",
    "            }\n",
    "            weather_records.append(record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(weather_records)\n",
    "\n",
    "weather_df = load_weather_data()\n",
    "print(f\"Weather data loaded: {len(weather_df)} records\")\n",
    "display(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retail data\n",
    "def load_retail_data():\n",
    "    try:\n",
    "        df = pd.read_csv(RETAIL_FILE)\n",
    "        print(f\"Retail data loaded: {len(df)} records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading retail data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "retail_df = load_retail_data()\n",
    "if not retail_df.empty:\n",
    "    display(retail_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01928833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load headlines data\n",
    "def load_headlines_data():\n",
    "    headlines_files = list(HEADLINES_DIR.glob(\"*.json\"))\n",
    "    headlines_records = []\n",
    "    \n",
    "    for file_path in headlines_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    record = {\n",
    "                        'title': item.get('title'),\n",
    "                        'description': item.get('description'),\n",
    "                        'url': item.get('url'),\n",
    "                        'published_at': item.get('publishedAt'),\n",
    "                        'source': item.get('source', {}).get('name'),\n",
    "                        'author': item.get('author'),\n",
    "                        'content': item.get('content'),\n",
    "                        'source_file': file_path.name\n",
    "                    }\n",
    "                    headlines_records.append(record)\n",
    "            else:\n",
    "                record = {\n",
    "                    'title': data.get('title'),\n",
    "                    'description': data.get('description'),\n",
    "                    'url': data.get('url'),\n",
    "                    'published_at': data.get('publishedAt'),\n",
    "                    'source': data.get('source', {}).get('name'),\n",
    "                    'author': data.get('author'),\n",
    "                    'content': data.get('content'),\n",
    "                    'source_file': file_path.name\n",
    "                }\n",
    "                headlines_records.append(record)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(headlines_records)\n",
    "\n",
    "headlines_df = load_headlines_data()\n",
    "print(f\"Headlines data loaded: {len(headlines_df)} records\")\n",
    "display(headlines_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ccf3d",
   "metadata": {},
   "source": [
    "## 2. Weather Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data transformation prototype\n",
    "def transform_weather_data(df):\n",
    "    \"\"\"Transform weather data - prototype version\"\"\"\n",
    "    transformed = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    transformed['date'] = pd.to_datetime(transformed['timestamp'], unit='s', errors='coerce').dt.normalize()\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = ['temperature', 'humidity', 'pressure', 'wind_speed', 'clouds']\n",
    "    for col in numeric_cols:\n",
    "        if col in transformed.columns:\n",
    "            transformed[col] = transformed[col].fillna(transformed[col].mean())\n",
    "    \n",
    "    # Handle categorical missing values\n",
    "    transformed['weather_main'] = transformed['weather_main'].fillna('Unknown')\n",
    "    transformed['weather_description'] = transformed['weather_description'].fillna('Unknown')\n",
    "    transformed['city'] = transformed['city'].fillna('Unknown')\n",
    "    \n",
    "    # Add derived columns\n",
    "    transformed['temp_category'] = pd.cut(\n",
    "        transformed['temperature'], \n",
    "        bins=[-50, 0, 15, 25, 35, 50], \n",
    "        labels=['Freezing', 'Cold', 'Cool', 'Warm', 'Hot']\n",
    "    )\n",
    "    \n",
    "    transformed['humidity_category'] = pd.cut(\n",
    "        transformed['humidity'], \n",
    "        bins=[0, 30, 60, 80, 100], \n",
    "        labels=['Dry', 'Normal', 'Humid', 'Very Humid']\n",
    "    )\n",
    "    \n",
    "    # Select final columns\n",
    "    final_columns = [\n",
    "        'date', 'city', 'temperature', 'humidity', 'pressure', \n",
    "        'weather_main', 'weather_description', 'wind_speed',\n",
    "        'temp_category', 'humidity_category'\n",
    "    ]\n",
    "    \n",
    "    return transformed[final_columns].dropna(subset=['date'])\n",
    "\n",
    "# Apply transformation\n",
    "weather_transformed = transform_weather_data(weather_df)\n",
    "print(f\"Weather data transformed: {len(weather_transformed)} records\")\n",
    "print(\"Transformed columns:\", weather_transformed.columns.tolist())\n",
    "display(weather_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate weather data to daily level (prototype)\n",
    "def aggregate_weather_daily(df):\n",
    "    \"\"\"Aggregate weather data to daily level\"\"\"\n",
    "    daily_agg = df.groupby(['date', 'city']).agg({\n",
    "        'temperature': ['mean', 'min', 'max'],\n",
    "        'humidity': 'mean',\n",
    "        'pressure': 'mean',\n",
    "        'wind_speed': 'mean',\n",
    "        'weather_main': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    daily_agg.columns = ['temp_mean', 'temp_min', 'temp_max', 'humidity_mean', 'pressure_mean', 'wind_speed_mean', 'weather_main_mode']\n",
    "    daily_agg = daily_agg.reset_index()\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "# Apply daily aggregation\n",
    "weather_daily = aggregate_weather_daily(weather_transformed)\n",
    "print(f\"Weather data aggregated to daily: {len(weather_daily)} records\")\n",
    "display(weather_daily.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd86c0",
   "metadata": {},
   "source": [
    "## 3. Retail Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29452519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retail data transformation prototype\n",
    "def transform_retail_data(df):\n",
    "    \"\"\"Transform retail data - prototype version\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    transformed = df.copy()\n",
    "    \n",
    "    # Identify date columns\n",
    "    date_cols = [col for col in transformed.columns if 'date' in col.lower()]\n",
    "    for col in date_cols:\n",
    "        try:\n",
    "            transformed[col] = pd.to_datetime(transformed[col], errors='coerce')\n",
    "            print(f\"Converted {col} to datetime\")\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to datetime\")\n",
    "    \n",
    "    # Handle missing values for numeric columns\n",
    "    numeric_cols = transformed.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        transformed[col] = transformed[col].fillna(transformed[col].median())\n",
    "    \n",
    "    # Handle categorical missing values\n",
    "    categorical_cols = transformed.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        transformed[col] = transformed[col].fillna('Unknown')\n",
    "    \n",
    "    # Standardize column names (lowercase, replace spaces with underscores)\n",
    "    transformed.columns = transformed.columns.str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Add derived columns (example)\n",
    "    # This would depend on actual column names in the retail data\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "# Apply transformation\n",
    "if not retail_df.empty:\n",
    "    retail_transformed = transform_retail_data(retail_df)\n",
    "    print(f\"Retail data transformed: {len(retail_transformed)} records\")\n",
    "    print(\"Transformed columns:\", retail_transformed.columns.tolist())\n",
    "    display(retail_transformed.head())\n",
    "else:\n",
    "    retail_transformed = pd.DataFrame()\n",
    "    print(\"No retail data to transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c70f2b",
   "metadata": {},
   "source": [
    "## 4. Headlines Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines data transformation prototype\n",
    "def transform_headlines_data(df):\n",
    "    \"\"\"Transform headlines data - prototype version\"\"\"\n",
    "    transformed = df.copy()\n",
    "    \n",
    "    # Convert published_at to datetime\n",
    "    transformed['published_at'] = pd.to_datetime(transformed['published_at'], errors='coerce')\n",
    "    transformed['date'] = transformed['published_at'].dt.normalize()\n",
    "    \n",
    "    # Clean text fields\n",
    "    text_cols = ['title', 'description', 'content']\n",
    "    for col in text_cols:\n",
    "        if col in transformed.columns:\n",
    "            transformed[col] = transformed[col].fillna('')\n",
    "            transformed[col] = transformed[col].str.strip()\n",
    "    \n",
    "    # Handle missing sources\n",
    "    transformed['source'] = transformed['source'].fillna('Unknown')\n",
    "    \n",
    "    # Add derived columns\n",
    "    if 'title' in transformed.columns:\n",
    "        transformed['title_length'] = transformed['title'].str.len()\n",
    "        transformed['has_description'] = transformed['description'].str.len() > 0\n",
    "    \n",
    "    # Filter out invalid records\n",
    "    valid_records = transformed.dropna(subset=['date', 'title'])\n",
    "    \n",
    "    # Select final columns\n",
    "    final_columns = [\n",
    "        'date', 'title', 'description', 'source', 'url', \n",
    "        'title_length', 'has_description'\n",
    "    ]\n",
    "    \n",
    "    return valid_records[final_columns]\n",
    "\n",
    "# Apply transformation\n",
    "headlines_transformed = transform_headlines_data(headlines_df)\n",
    "print(f\"Headlines data transformed: {len(headlines_transformed)} records\")\n",
    "print(\"Transformed columns:\", headlines_transformed.columns.tolist())\n",
    "display(headlines_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ab195",
   "metadata": {},
   "source": [
    "## 5. Data Integration Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data integration prototype\n",
    "def integrate_datasets(weather_df, retail_df, headlines_df):\n",
    "    \"\"\"Integrate the three datasets using merge_asof\"\"\"\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    for df in [weather_df, retail_df, headlines_df]:\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # Sort by date for merge_asof\n",
    "    weather_sorted = weather_df.dropna(subset=['date']).sort_values('date')\n",
    "    retail_sorted = retail_df.dropna(subset=['date']).sort_values('date') if not retail_df.empty else pd.DataFrame()\n",
    "    headlines_sorted = headlines_df.dropna(subset=['date']).sort_values('date')\n",
    "    \n",
    "    print(f\"Weather records: {len(weather_sorted)}\")\n",
    "    print(f\"Retail records: {len(retail_sorted)}\")\n",
    "    print(f\"Headlines records: {len(headlines_sorted)}\")\n",
    "    \n",
    "    # Start with retail data as base (if available)\n",
    "    if not retail_sorted.empty:\n",
    "        # Merge retail with nearest weather record\n",
    "        integrated = pd.merge_asof(\n",
    "            retail_sorted,\n",
    "            weather_sorted,\n",
    "            on='date',\n",
    "            direction='nearest',\n",
    "            suffixes=('_retail', '_weather')\n",
    "        )\n",
    "        \n",
    "        # Merge with headlines\n",
    "        integrated = pd.merge_asof(\n",
    "            integrated,\n",
    "            headlines_sorted,\n",
    "            on='date',\n",
    "            direction='nearest',\n",
    "            suffixes=('', '_headlines')\n",
    "        )\n",
    "    else:\n",
    "        # If no retail data, integrate weather and headlines\n",
    "        integrated = pd.merge_asof(\n",
    "            weather_sorted,\n",
    "            headlines_sorted,\n",
    "            on='date',\n",
    "            direction='nearest',\n",
    "            suffixes=('', '_headlines')\n",
    "        )\n",
    "    \n",
    "    return integrated\n",
    "\n",
    "# Apply integration\n",
    "integrated_data = integrate_datasets(weather_daily, retail_transformed, headlines_transformed)\n",
    "print(f\"Integrated data: {len(integrated_data)} records\")\n",
    "print(\"Integrated columns:\", integrated_data.columns.tolist())\n",
    "display(integrated_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a23ec3",
   "metadata": {},
   "source": [
    "## 6. Validation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with existing processed data\n",
    "def compare_with_existing(processed_file, new_data):\n",
    "    \"\"\"Compare new transformation results with existing processed data\"\"\"\n",
    "    try:\n",
    "        existing_data = pd.read_parquet(processed_file)\n",
    "        print(f\"Existing data: {len(existing_data)} records\")\n",
    "        print(f\"New data: {len(new_data)} records\")\n",
    "        \n",
    "        print(\"\\nExisting data columns:\", existing_data.columns.tolist())\n",
    "        print(\"New data columns:\", new_data.columns.tolist())\n",
    "        \n",
    "        # Compare column overlap\n",
    "        common_cols = set(existing_data.columns) & set(new_data.columns)\n",
    "        print(f\"Common columns: {len(common_cols)}\")\n",
    "        \n",
    "        return existing_data, common_cols\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load existing data: {e}\")\n",
    "        return None, set()\n",
    "\n",
    "# Check for existing processed files\n",
    "processed_files = list(PROCESSED_DIR.glob(\"*.parquet\"))\n",
    "if processed_files:\n",
    "    print(\"Existing processed files:\")\n",
    "    for file in processed_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "        \n",
    "    # Compare with integrated dataset if it exists\n",
    "    integrated_file = PROCESSED_DIR / \"integrated_dataset.parquet\"\n",
    "    if integrated_file.exists():\n",
    "        existing_integrated, common_cols = compare_with_existing(integrated_file, integrated_data)\n",
    "else:\n",
    "    print(\"No existing processed files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c83732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "def validate_transformations(df, name):\n",
    "    \"\"\"Validate transformation results\"\"\"\n",
    "    print(f\"\\n{name} Validation:\")\n",
    "    print(f\"Records: {len(df)}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicate records: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Date validation\n",
    "    if 'date' in df.columns:\n",
    "        valid_dates = pd.to_datetime(df['date'], errors='coerce').notnull().sum()\n",
    "        print(f\"Valid dates: {valid_dates}/{len(df)}\")\n",
    "    \n",
    "    # Numeric validation\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "        for col in numeric_cols[:3]:  # Check first 3\n",
    "            outliers = detect_outliers_iqr(df, col)\n",
    "            print(f\"  {col}: {outliers.sum()} outliers\")\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    if column not in df.columns or df[column].dtype not in ['int64', 'float64']:\n",
    "        return pd.Series([], dtype=bool)\n",
    "    \n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "# Validate all transformations\n",
    "validate_transformations(weather_daily, \"Weather\")\n",
    "if not retail_transformed.empty:\n",
    "    validate_transformations(retail_transformed, \"Retail\")\n",
    "validate_transformations(headlines_transformed, \"Headlines\")\n",
    "validate_transformations(integrated_data, \"Integrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c4262",
   "metadata": {},
   "source": [
    "## 7. Export Prototyped Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14463846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export transformed data for testing\n",
    "def export_prototypes():\n",
    "    \"\"\"Export transformed data to test files\"\"\"\n",
    "    output_dir = Path(\"../data/test_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export weather data\n",
    "    weather_output = output_dir / \"weather_prototype.parquet\"\n",
    "    weather_daily.to_parquet(weather_output, index=False)\n",
    "    print(f\"Weather prototype exported: {weather_output}\")\n",
    "    \n",
    "    # Export retail data\n",
    "    if not retail_transformed.empty:\n",
    "        retail_output = output_dir / \"retail_prototype.parquet\"\n",
    "        retail_transformed.to_parquet(retail_output, index=False)\n",
    "        print(f\"Retail prototype exported: {retail_output}\")\n",
    "    \n",
    "    # Export headlines data\n",
    "    headlines_output = output_dir / \"headlines_prototype.parquet\"\n",
    "    headlines_transformed.to_parquet(headlines_output, index=False)\n",
    "    print(f\"Headlines prototype exported: {headlines_output}\")\n",
    "    \n",
    "    # Export integrated data\n",
    "    integrated_output = output_dir / \"integrated_prototype.parquet\"\n",
    "    integrated_data.to_parquet(integrated_output, index=False)\n",
    "    print(f\"Integrated prototype exported: {integrated_output}\")\n",
    "    \n",
    "    print(f\"\\nAll prototype files exported to: {output_dir.absolute()}\")\n",
    "\n",
    "# Export the prototypes\n",
    "export_prototypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a53ac",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ETL PROTOTYPING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Weather data: {len(weather_df)} raw → {len(weather_daily)} processed\")\n",
    "if not retail_transformed.empty:\n",
    "    print(f\"Retail data: {len(retail_df)} raw → {len(retail_transformed)} processed\")\n",
    "else:\n",
    "    print(\"Retail data: No data available\")\n",
    "print(f\"Headlines data: {len(headlines_df)} raw → {len(headlines_transformed)} processed\")\n",
    "print(f\"Integrated data: {len(integrated_data)} records\")\n",
    "\n",
    "print(\"\\nTRANSFORMATION FEATURES TESTED:\")\n",
    "print(\"✓ Date parsing and normalization\")\n",
    "print(\"✓ Missing value handling\")\n",
    "print(\"✓ Data aggregation (weather daily)\")\n",
    "print(\"✓ Categorical encoding\")\n",
    "print(\"✓ Data integration with merge_asof\")\n",
    "print(\"✓ Derived column creation\")\n",
    "\n",
    "print(\"\\nVALIDATION RESULTS:\")\n",
    "print(f\"Weather missing values: {weather_daily.isnull().sum().sum()}\")\n",
    "if not retail_transformed.empty:\n",
    "    print(f\"Retail missing values: {retail_transformed.isnull().sum().sum()}\")\n",
    "print(f\"Headlines missing values: {headlines_transformed.isnull().sum().sum()}\")\n",
    "print(f\"Integrated missing values: {integrated_data.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"1. Review transformation logic and adjust as needed\")\n",
    "print(\"2. Test edge cases and error handling\")\n",
    "print(\"3. Compare with existing production transformations\")\n",
    "print(\"4. Implement approved changes in production scripts\")\n",
    "print(\"5. Add unit tests for transformation functions\")\n",
    "print(\"6. Document transformation rules and business logic\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
