{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49776d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "RAW_DATA_DIR = Path(\"../data/raw\")\n",
    "WEATHER_DIR = RAW_DATA_DIR\n",
    "RETAIL_FILE = RAW_DATA_DIR / \"retail_sales_dataset.csv\"\n",
    "HEADLINES_DIR = RAW_DATA_DIR / \"web\"\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR.absolute()}\")\n",
    "print(f\"Weather files location: {WEATHER_DIR}\")\n",
    "print(f\"Retail data: {RETAIL_FILE}\")\n",
    "print(f\"Headlines data: {HEADLINES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442eedec",
   "metadata": {},
   "source": [
    "## 1. Weather Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List weather JSON files\n",
    "weather_files = list(WEATHER_DIR.glob(\"*_weather_*.json\"))\n",
    "print(f\"Found {len(weather_files)} weather data files:\")\n",
    "for file in weather_files:\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# Load and examine first weather file\n",
    "if weather_files:\n",
    "    sample_weather_file = weather_files[0]\n",
    "    print(f\"\\nExamining sample file: {sample_weather_file.name}\")\n",
    "    \n",
    "    with open(sample_weather_file, 'r') as f:\n",
    "        sample_weather = json.load(f)\n",
    "    \n",
    "    print(\"Sample weather data structure:\")\n",
    "    print(json.dumps(sample_weather, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all weather data into a DataFrame\n",
    "weather_records = []\n",
    "\n",
    "for file_path in weather_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract relevant fields\n",
    "        record = {\n",
    "            'timestamp': data.get('dt'),\n",
    "            'date': pd.to_datetime(data.get('dt'), unit='s').normalize() if data.get('dt') else None,\n",
    "            'city': data.get('name'),\n",
    "            'country': data.get('sys', {}).get('country'),\n",
    "            'temperature': data.get('main', {}).get('temp'),\n",
    "            'feels_like': data.get('main', {}).get('feels_like'),\n",
    "            'humidity': data.get('main', {}).get('humidity'),\n",
    "            'pressure': data.get('main', {}).get('pressure'),\n",
    "            'weather_main': data.get('weather', [{}])[0].get('main'),\n",
    "            'weather_description': data.get('weather', [{}])[0].get('description'),\n",
    "            'wind_speed': data.get('wind', {}).get('speed'),\n",
    "            'wind_direction': data.get('wind', {}).get('deg'),\n",
    "            'clouds': data.get('clouds', {}).get('all'),\n",
    "            'source_file': file_path.name\n",
    "        }\n",
    "        weather_records.append(record)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "weather_df = pd.DataFrame(weather_records)\n",
    "print(f\"\\nWeather data shape: {weather_df.shape}\")\n",
    "print(\"\\nWeather data info:\")\n",
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319321f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data statistics\n",
    "print(\"Weather Data Summary:\")\n",
    "display(weather_df.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "display(weather_df.isnull().sum())\n",
    "\n",
    "print(\"\\nUnique cities:\")\n",
    "display(weather_df['city'].value_counts())\n",
    "\n",
    "print(\"\\nWeather conditions:\")\n",
    "display(weather_df['weather_main'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Weather Data Exploration', fontsize=16)\n",
    "\n",
    "# Temperature distribution\n",
    "axes[0,0].hist(weather_df['temperature'].dropna(), bins=20, alpha=0.7)\n",
    "axes[0,0].set_title('Temperature Distribution')\n",
    "axes[0,0].set_xlabel('Temperature (Â°C)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Humidity distribution\n",
    "axes[0,1].hist(weather_df['humidity'].dropna(), bins=20, alpha=0.7)\n",
    "axes[0,1].set_title('Humidity Distribution')\n",
    "axes[0,1].set_xlabel('Humidity (%)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Cities\n",
    "weather_df['city'].value_counts().plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Weather Data by City')\n",
    "axes[1,0].set_xlabel('City')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Weather conditions\n",
    "weather_df['weather_main'].value_counts().plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Weather Conditions')\n",
    "axes[1,1].set_xlabel('Condition')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abeb2d2",
   "metadata": {},
   "source": [
    "## 2. Retail Sales Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ee139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retail sales data\n",
    "try:\n",
    "    retail_df = pd.read_csv(RETAIL_FILE)\n",
    "    print(f\"Retail data shape: {retail_df.shape}\")\n",
    "    print(\"\\nRetail data columns:\")\n",
    "    print(retail_df.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(retail_df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Retail data file not found: {RETAIL_FILE}\")\n",
    "    retail_df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading retail data: {e}\")\n",
    "    retail_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9834a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retail_df is not None:\n",
    "    print(\"Retail Data Summary:\")\n",
    "    display(retail_df.describe())\n",
    "    \n",
    "    print(\"\\nData types:\")\n",
    "    display(retail_df.dtypes)\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    display(retail_df.isnull().sum())\n",
    "    \n",
    "    # Check for date columns\n",
    "    date_columns = [col for col in retail_df.columns if 'date' in col.lower()]\n",
    "    if date_columns:\n",
    "        print(f\"\\nPotential date columns: {date_columns}\")\n",
    "        for col in date_columns:\n",
    "            try:\n",
    "                retail_df[col] = pd.to_datetime(retail_df[col])\n",
    "                print(f\"Converted {col} to datetime\")\n",
    "            except:\n",
    "                print(f\"Could not convert {col} to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retail_df is not None:\n",
    "    # Retail data visualizations\n",
    "    numeric_cols = retail_df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        fig, axes = plt.subplots(1, min(3, len(numeric_cols)), figsize=(15, 5))\n",
    "        fig.suptitle('Retail Data Numeric Distributions', fontsize=16)\n",
    "        \n",
    "        if len(numeric_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:3]):\n",
    "            if i < len(axes):\n",
    "                retail_df[col].hist(bins=20, ax=axes[i], alpha=0.7)\n",
    "                axes[i].set_title(f'{col} Distribution')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Categorical columns\n",
    "    categorical_cols = retail_df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\nCategorical columns value counts:\")\n",
    "        for col in categorical_cols[:3]:  # Show first 3 categorical columns\n",
    "            print(f\"\\n{col}:\")\n",
    "            display(retail_df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353956db",
   "metadata": {},
   "source": [
    "## 3. News Headlines Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb226e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List headlines JSON files\n",
    "headlines_files = list(HEADLINES_DIR.glob(\"*.json\"))\n",
    "print(f\"Found {len(headlines_files)} headlines data files:\")\n",
    "for file in headlines_files:\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "# Load and examine headlines data\n",
    "headlines_records = []\n",
    "\n",
    "for file_path in headlines_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract relevant fields (adjust based on actual structure)\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                record = {\n",
    "                    'title': item.get('title'),\n",
    "                    'description': item.get('description'),\n",
    "                    'url': item.get('url'),\n",
    "                    'published_at': item.get('publishedAt'),\n",
    "                    'source': item.get('source', {}).get('name'),\n",
    "                    'author': item.get('author'),\n",
    "                    'source_file': file_path.name\n",
    "                }\n",
    "                headlines_records.append(record)\n",
    "        else:\n",
    "            # Single object structure\n",
    "            record = {\n",
    "                'title': data.get('title'),\n",
    "                'description': data.get('description'),\n",
    "                'url': data.get('url'),\n",
    "                'published_at': data.get('publishedAt'),\n",
    "                'source': data.get('source', {}).get('name'),\n",
    "                'author': data.get('author'),\n",
    "                'source_file': file_path.name\n",
    "            }\n",
    "            headlines_records.append(record)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "headlines_df = pd.DataFrame(headlines_records)\n",
    "print(f\"\\nHeadlines data shape: {headlines_df.shape}\")\n",
    "print(\"\\nHeadlines data info:\")\n",
    "headlines_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not headlines_df.empty:\n",
    "    print(\"Headlines Data Summary:\")\n",
    "    display(headlines_df.describe())\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    display(headlines_df.isnull().sum())\n",
    "    \n",
    "    print(\"\\nTop sources:\")\n",
    "    display(headlines_df['source'].value_counts().head(10))\n",
    "    \n",
    "    # Convert published_at to datetime if possible\n",
    "    if 'published_at' in headlines_df.columns:\n",
    "        try:\n",
    "            headlines_df['published_at'] = pd.to_datetime(headlines_df['published_at'])\n",
    "            print(\"\\nConverted published_at to datetime\")\n",
    "            print(f\"Date range: {headlines_df['published_at'].min()} to {headlines_df['published_at'].max()}\")\n",
    "        except:\n",
    "            print(\"\\nCould not convert published_at to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dcc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not headlines_df.empty:\n",
    "    # Headlines data visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle('Headlines Data Exploration', fontsize=16)\n",
    "    \n",
    "    # Sources distribution\n",
    "    headlines_df['source'].value_counts().head(10).plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Top News Sources')\n",
    "    axes[0].set_xlabel('Source')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Publication timeline (if datetime conversion worked)\n",
    "    if 'published_at' in headlines_df.columns and pd.api.types.is_datetime64_any_dtype(headlines_df['published_at']):\n",
    "        headlines_df['published_at'].dt.date.value_counts().sort_index().plot(ax=axes[1])\n",
    "        axes[1].set_title('Headlines Over Time')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Number of Headlines')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No datetime data available', \n",
    "                    transform=axes[1].transAxes, ha='center', va='center')\n",
    "        axes[1].set_title('Publication Timeline')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602b87b",
   "metadata": {},
   "source": [
    "## 4. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics across all datasets\n",
    "summary_data = {\n",
    "    'Dataset': ['Weather', 'Retail', 'Headlines'],\n",
    "    'Records': [len(weather_df), len(retail_df) if retail_df is not None else 0, len(headlines_df)],\n",
    "    'Columns': [len(weather_df.columns), len(retail_df.columns) if retail_df is not None else 0, len(headlines_df.columns)]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Dataset Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Data quality issues identified\n",
    "print(\"\\nData Quality Notes:\")\n",
    "print(\"1. Weather data:\")\n",
    "print(f\"   - Missing values: {weather_df.isnull().sum().sum()} total\")\n",
    "print(f\"   - Cities covered: {weather_df['city'].nunique()}\")\n",
    "\n",
    "if retail_df is not None:\n",
    "    print(\"2. Retail data:\")\n",
    "    print(f\"   - Missing values: {retail_df.isnull().sum().sum()} total\")\n",
    "    print(f\"   - Numeric columns: {len(retail_df.select_dtypes(include=['number']).columns)}\")\n",
    "\n",
    "print(\"3. Headlines data:\")\n",
    "print(f\"   - Missing values: {headlines_df.isnull().sum().sum()} total\")\n",
    "print(f\"   - Sources: {headlines_df['source'].nunique() if not headlines_df.empty else 0}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Review missing value handling strategies\")\n",
    "print(\"- Standardize date formats across datasets\")\n",
    "print(\"- Validate data ranges and business rules\")\n",
    "print(\"- Consider data enrichment opportunities\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
