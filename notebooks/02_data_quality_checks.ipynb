{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "RAW_DATA_DIR = Path(\"../data/raw\")\n",
    "WEATHER_DIR = RAW_DATA_DIR\n",
    "RETAIL_FILE = RAW_DATA_DIR / \"retail_sales_dataset.csv\"\n",
    "HEADLINES_DIR = RAW_DATA_DIR / \"web\"\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment functions\n",
    "def completeness_score(df):\n",
    "    \"\"\"Calculate completeness score (percentage of non-null values)\"\"\"\n",
    "    return (df.notnull().sum() / len(df) * 100).round(2)\n",
    "\n",
    "def uniqueness_score(df, column):\n",
    "    \"\"\"Calculate uniqueness score for a column\"\"\"\n",
    "    if column in df.columns:\n",
    "        unique_ratio = df[column].nunique() / len(df) * 100\n",
    "        return round(unique_ratio, 2)\n",
    "    return 0\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    if column not in df.columns or df[column].dtype not in ['int64', 'float64']:\n",
    "        return pd.Series([], dtype=bool)\n",
    "    \n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "\n",
    "def data_quality_report(df, name):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    report = {\n",
    "        'dataset': name,\n",
    "        'total_records': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'completeness_overall': round(df.notnull().sum().sum() / (len(df) * len(df.columns)) * 100, 2),\n",
    "        'columns_with_nulls': (df.isnull().sum() > 0).sum(),\n",
    "        'duplicate_records': df.duplicated().sum()\n",
    "    }\n",
    "    \n",
    "    # Column-level metrics\n",
    "    column_metrics = []\n",
    "    for col in df.columns:\n",
    "        col_info = {\n",
    "            'column': col,\n",
    "            'dtype': str(df[col].dtype),\n",
    "            'completeness': round(df[col].notnull().sum() / len(df) * 100, 2),\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'null_count': df[col].isnull().sum()\n",
    "        }\n",
    "        \n",
    "        # Numeric columns - add outlier detection\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            outliers = detect_outliers_iqr(df, col)\n",
    "            col_info['outliers'] = outliers.sum()\n",
    "            col_info['outlier_percentage'] = round(outliers.sum() / len(df) * 100, 2)\n",
    "        else:\n",
    "            col_info['outliers'] = 0\n",
    "            col_info['outlier_percentage'] = 0\n",
    "            \n",
    "        column_metrics.append(col_info)\n",
    "    \n",
    "    return report, pd.DataFrame(column_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd026d8",
   "metadata": {},
   "source": [
    "## 1. Weather Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fad4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_files = list(WEATHER_DIR.glob(\"*_weather_*.json\"))\n",
    "weather_records = []\n",
    "\n",
    "for file_path in weather_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        record = {\n",
    "            'timestamp': data.get('dt'),\n",
    "            'date': pd.to_datetime(data.get('dt'), unit='s').normalize() if data.get('dt') else None,\n",
    "            'city': data.get('name'),\n",
    "            'temperature': data.get('main', {}).get('temp'),\n",
    "            'humidity': data.get('main', {}).get('humidity'),\n",
    "            'pressure': data.get('main', {}).get('pressure'),\n",
    "            'weather_main': data.get('weather', [{}])[0].get('main'),\n",
    "            'wind_speed': data.get('wind', {}).get('speed'),\n",
    "            'source_file': file_path.name\n",
    "        }\n",
    "        weather_records.append(record)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "weather_df = pd.DataFrame(weather_records)\n",
    "print(f\"Weather data loaded: {len(weather_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data quality report\n",
    "weather_report, weather_column_metrics = data_quality_report(weather_df, 'Weather')\n",
    "\n",
    "print(\"Weather Data Quality Report:\")\n",
    "for key, value in weather_report.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nColumn-level Quality Metrics:\")\n",
    "display(weather_column_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b611c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data quality visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Weather Data Quality Analysis', fontsize=16)\n",
    "\n",
    "# Completeness by column\n",
    "weather_column_metrics.set_index('column')['completeness'].plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Data Completeness by Column')\n",
    "axes[0,0].set_ylabel('Completeness (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Missing values heatmap\n",
    "sns.heatmap(weather_df.isnull(), cbar=False, ax=axes[0,1])\n",
    "axes[0,1].set_title('Missing Values Pattern')\n",
    "axes[0,1].set_xlabel('Columns')\n",
    "axes[0,1].set_ylabel('Records')\n",
    "\n",
    "# Outlier analysis for numeric columns\n",
    "numeric_cols = weather_column_metrics[weather_column_metrics['dtype'].isin(['int64', 'float64'])]\n",
    "if not numeric_cols.empty:\n",
    "    numeric_cols.set_index('column')['outlier_percentage'].plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Outlier Percentage by Numeric Column')\n",
    "    axes[1,0].set_ylabel('Outlier Percentage (%)')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'No numeric columns', transform=axes[1,0].transAxes, ha='center')\n",
    "\n",
    "# Duplicate analysis\n",
    "duplicate_info = f\"Duplicate records: {weather_report['duplicate_records']}\"\n",
    "axes[1,1].text(0.5, 0.5, duplicate_info, transform=axes[1,1].transAxes, ha='center', fontsize=12)\n",
    "axes[1,1].set_title('Duplicate Records')\n",
    "axes[1,1].set_xlim(0, 1)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d513619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data validation rules\n",
    "print(\"Weather Data Validation Results:\")\n",
    "\n",
    "# Temperature range check (-50°C to 60°C)\n",
    "if 'temperature' in weather_df.columns:\n",
    "    temp_valid = ((weather_df['temperature'] >= -50) & (weather_df['temperature'] <= 60)).sum()\n",
    "    temp_invalid = len(weather_df) - temp_valid\n",
    "    print(f\"Temperature range validation: {temp_valid} valid, {temp_invalid} invalid\")\n",
    "\n",
    "# Humidity range check (0-100%)\n",
    "if 'humidity' in weather_df.columns:\n",
    "    humidity_valid = ((weather_df['humidity'] >= 0) & (weather_df['humidity'] <= 100)).sum()\n",
    "    humidity_invalid = len(weather_df) - humidity_valid\n",
    "    print(f\"Humidity range validation: {humidity_valid} valid, {humidity_invalid} invalid\")\n",
    "\n",
    "# Pressure range check (800-1200 hPa)\n",
    "if 'pressure' in weather_df.columns:\n",
    "    pressure_valid = ((weather_df['pressure'] >= 800) & (weather_df['pressure'] <= 1200)).sum()\n",
    "    pressure_invalid = len(weather_df) - pressure_valid\n",
    "    print(f\"Pressure range validation: {pressure_valid} valid, {pressure_invalid} invalid\")\n",
    "\n",
    "# Date validation\n",
    "if 'date' in weather_df.columns:\n",
    "    future_dates = (weather_df['date'] > pd.Timestamp.now()).sum()\n",
    "    old_dates = (weather_df['date'] < pd.Timestamp('2020-01-01')).sum()\n",
    "    print(f\"Date validation: {future_dates} future dates, {old_dates} very old dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2d063",
   "metadata": {},
   "source": [
    "## 2. Retail Sales Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2385da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retail data\n",
    "try:\n",
    "    retail_df = pd.read_csv(RETAIL_FILE)\n",
    "    print(f\"Retail data loaded: {len(retail_df)} records\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading retail data: {e}\")\n",
    "    retail_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f73131",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not retail_df.empty:\n",
    "    # Retail data quality report\n",
    "    retail_report, retail_column_metrics = data_quality_report(retail_df, 'Retail')\n",
    "    \n",
    "    print(\"Retail Data Quality Report:\")\n",
    "    for key, value in retail_report.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\nColumn-level Quality Metrics:\")\n",
    "    display(retail_column_metrics)\n",
    "else:\n",
    "    print(\"No retail data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not retail_df.empty:\n",
    "    # Retail data quality visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Retail Data Quality Analysis', fontsize=16)\n",
    "    \n",
    "    # Completeness by column\n",
    "    retail_column_metrics.set_index('column')['completeness'].plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Data Completeness by Column')\n",
    "    axes[0,0].set_ylabel('Completeness (%)')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Missing values heatmap\n",
    "    sns.heatmap(retail_df.isnull(), cbar=False, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Missing Values Pattern')\n",
    "    axes[0,1].set_xlabel('Columns')\n",
    "    axes[0,1].set_ylabel('Records')\n",
    "    \n",
    "    # Outlier analysis for numeric columns\n",
    "    numeric_cols = retail_column_metrics[retail_column_metrics['dtype'].isin(['int64', 'float64'])]\n",
    "    if not numeric_cols.empty:\n",
    "        numeric_cols.set_index('column')['outlier_percentage'].plot(kind='bar', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Outlier Percentage by Numeric Column')\n",
    "        axes[1,0].set_ylabel('Outlier Percentage (%)')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'No numeric columns', transform=axes[1,0].transAxes, ha='center')\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    duplicate_info = f\"Duplicate records: {retail_report['duplicate_records']}\"\n",
    "    axes[1,1].text(0.5, 0.5, duplicate_info, transform=axes[1,1].transAxes, ha='center', fontsize=12)\n",
    "    axes[1,1].set_title('Duplicate Records')\n",
    "    axes[1,1].set_xlim(0, 1)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e39ce",
   "metadata": {},
   "source": [
    "## 3. Headlines Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load headlines data\n",
    "headlines_files = list(HEADLINES_DIR.glob(\"*.json\"))\n",
    "headlines_records = []\n",
    "\n",
    "for file_path in headlines_files:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                record = {\n",
    "                    'title': item.get('title'),\n",
    "                    'description': item.get('description'),\n",
    "                    'published_at': item.get('publishedAt'),\n",
    "                    'source': item.get('source', {}).get('name'),\n",
    "                    'url': item.get('url'),\n",
    "                    'source_file': file_path.name\n",
    "                }\n",
    "                headlines_records.append(record)\n",
    "        else:\n",
    "            record = {\n",
    "                'title': data.get('title'),\n",
    "                'description': data.get('description'),\n",
    "                'published_at': data.get('publishedAt'),\n",
    "                'source': data.get('source', {}).get('name'),\n",
    "                'url': data.get('url'),\n",
    "                'source_file': file_path.name\n",
    "            }\n",
    "            headlines_records.append(record)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "headlines_df = pd.DataFrame(headlines_records)\n",
    "print(f\"Headlines data loaded: {len(headlines_df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc41e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines data quality report\n",
    "headlines_report, headlines_column_metrics = data_quality_report(headlines_df, 'Headlines')\n",
    "\n",
    "print(\"Headlines Data Quality Report:\")\n",
    "for key, value in headlines_report.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nColumn-level Quality Metrics:\")\n",
    "display(headlines_column_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f016779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines data quality visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Headlines Data Quality Analysis', fontsize=16)\n",
    "\n",
    "# Completeness by column\n",
    "headlines_column_metrics.set_index('column')['completeness'].plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Data Completeness by Column')\n",
    "axes[0,0].set_ylabel('Completeness (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Missing values heatmap\n",
    "sns.heatmap(headlines_df.isnull(), cbar=False, ax=axes[0,1])\n",
    "axes[0,1].set_title('Missing Values Pattern')\n",
    "axes[0,1].set_xlabel('Columns')\n",
    "axes[0,1].set_ylabel('Records')\n",
    "\n",
    "# Title length distribution (if titles exist)\n",
    "if 'title' in headlines_df.columns:\n",
    "    title_lengths = headlines_df['title'].dropna().str.len()\n",
    "    title_lengths.hist(bins=20, ax=axes[1,0], alpha=0.7)\n",
    "    axes[1,0].set_title('Title Length Distribution')\n",
    "    axes[1,0].set_xlabel('Title Length (characters)')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'No title column', transform=axes[1,0].transAxes, ha='center')\n",
    "\n",
    "# Duplicate analysis\n",
    "duplicate_info = f\"Duplicate records: {headlines_report['duplicate_records']}\"\n",
    "axes[1,1].text(0.5, 0.5, duplicate_info, transform=axes[1,1].transAxes, ha='center', fontsize=12)\n",
    "axes[1,1].set_title('Duplicate Records')\n",
    "axes[1,1].set_xlim(0, 1)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "axes[1,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ac4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines validation rules\n",
    "print(\"Headlines Data Validation Results:\")\n",
    "\n",
    "# URL validation\n",
    "if 'url' in headlines_df.columns:\n",
    "    valid_urls = headlines_df['url'].str.startswith(('http://', 'https://')).sum()\n",
    "    invalid_urls = len(headlines_df) - valid_urls\n",
    "    print(f\"URL format validation: {valid_urls} valid, {invalid_urls} invalid\")\n",
    "\n",
    "# Title validation (non-empty)\n",
    "if 'title' in headlines_df.columns:\n",
    "    empty_titles = headlines_df['title'].str.strip().eq('').sum()\n",
    "    print(f\"Title validation: {empty_titles} empty titles\")\n",
    "\n",
    "# Date validation\n",
    "if 'published_at' in headlines_df.columns:\n",
    "    try:\n",
    "        dates = pd.to_datetime(headlines_df['published_at'], errors='coerce')\n",
    "        future_dates = (dates > pd.Timestamp.now()).sum()\n",
    "        old_dates = (dates < pd.Timestamp('2020-01-01')).sum()\n",
    "        invalid_dates = dates.isnull().sum()\n",
    "        print(f\"Date validation: {future_dates} future dates, {old_dates} very old dates, {invalid_dates} invalid formats\")\n",
    "    except:\n",
    "        print(\"Could not validate dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47706a",
   "metadata": {},
   "source": [
    "## 4. Cross-Dataset Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset quality comparison\n",
    "datasets = []\n",
    "if 'weather_report' in locals():\n",
    "    datasets.append(weather_report)\n",
    "if 'retail_report' in locals() and retail_report['total_records'] > 0:\n",
    "    datasets.append(retail_report)\n",
    "if 'headlines_report' in locals():\n",
    "    datasets.append(headlines_report)\n",
    "\n",
    "if datasets:\n",
    "    comparison_df = pd.DataFrame(datasets)\n",
    "    print(\"Cross-Dataset Quality Comparison:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Quality scores visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    comparison_df.set_index('dataset')['completeness_overall'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Overall Data Completeness by Dataset')\n",
    "    ax.set_ylabel('Completeness (%)')\n",
    "    ax.set_xlabel('Dataset')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No datasets available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e632571",
   "metadata": {},
   "source": [
    "## 5. Data Quality Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA QUALITY RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Weather data recommendations\n",
    "if 'weather_report' in locals():\n",
    "    print(\"\\nWEATHER DATA:\")\n",
    "    if weather_report['columns_with_nulls'] > 0:\n",
    "        print(f\n",
    ")\n",
    "        null_cols = weather_column_metrics[weather_column_metrics['null_count'] > 0]['column'].tolist()\n",
    "        print(f\"  Columns: {', '.join(null_cols)}\")\n",
    "    \n",
    "    outliers = weather_column_metrics[weather_column_metrics['outliers'] > 0]\n",
    "    if not outliers.empty:\n",
    "        print(f\"- Review {len(outliers)} columns with potential outliers\")\n",
    "        print(f\"  Columns: {', '.join(outliers['column'].tolist())}\")\n",
    "    \n",
    "    if weather_report['duplicate_records'] > 0:\n",
    "        print(f\"- Remove {weather_report['duplicate_records']} duplicate records\")\n",
    "\n",
    "# Retail data recommendations\n",
    "if 'retail_report' in locals() and retail_report['total_records'] > 0:\n",
    "    print(\"\\nRETAIL DATA:\")\n",
    "    if retail_report['columns_with_nulls'] > 0:\n",
    "        print(f\"- Handle {retail_report['columns_with_nulls']} columns with missing values\")\n",
    "        null_cols = retail_column_metrics[retail_column_metrics['null_count'] > 0]['column'].tolist()\n",
    "        print(f\"  Columns: {', '.join(null_cols)}\")\n",
    "    \n",
    "    outliers = retail_column_metrics[retail_column_metrics['outliers'] > 0]\n",
    "    if not outliers.empty:\n",
    "        print(f\"- Review {len(outliers)} columns with potential outliers\")\n",
    "        print(f\"  Columns: {', '.join(outliers['column'].tolist())}\")\n",
    "    \n",
    "    if retail_report['duplicate_records'] > 0:\n",
    "        print(f\"- Remove {retail_report['duplicate_records']} duplicate records\")\n",
    "\n",
    "# Headlines data recommendations\n",
    "if 'headlines_report' in locals():\n",
    "    print(\"\\nHEADLINES DATA:\")\n",
    "    if headlines_report['columns_with_nulls'] > 0:\n",
    "        print(f\"- Handle {headlines_report['columns_with_nulls']} columns with missing values\")\n",
    "        null_cols = headlines_column_metrics[headlines_column_metrics['null_count'] > 0]['column'].tolist()\n",
    "        print(f\"  Columns: {', '.join(null_cols)}\")\n",
    "    \n",
    "    if headlines_report['duplicate_records'] > 0:\n",
    "        print(f\"- Remove {headlines_report['duplicate_records']} duplicate records\")\n",
    "\n",
    "print(\"\\nGENERAL RECOMMENDATIONS:\")\n",
    "print(\"- Standardize date formats across all datasets\")\n",
    "print(\"- Implement data validation rules in ETL pipeline\")\n",
    "print(\"- Set up automated data quality monitoring\")\n",
    "print(\"- Document data quality expectations and thresholds\")\n",
    "print(\"- Consider data profiling tools for ongoing monitoring\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
